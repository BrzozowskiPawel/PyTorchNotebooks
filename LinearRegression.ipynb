{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EsHMhSqrnX4"
      },
      "source": [
        "## **Linear Regression**\n",
        "This notebook presents the issue of linear regression along with the explanation of individual sections, enabling an in-depth analysis of this issue. This course is based on aakashns's PyTorch for Deep Learning tutorial [linear-regression](https://jovian.ai/aakashns/02-linear-regression).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf4q75nKqwwN"
      },
      "source": [
        "\n",
        "First we need to install the required libraries. The installation of **PyTorch** may differ based on your operating system or hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFj23_C9q1ZZ",
        "outputId": "435b992f-bc2f-4eda-c652-1b023410efbb"
      },
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "\n",
        "# Linux / Binder / Colab\n",
        "!pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Windows\n",
        "# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS\n",
        "# !pip install numpy torch torchvision torchaudio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting torch==1.7.0+cpu\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-1.7.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (159.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 159.3 MB 4.5 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.1+cpu\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.8.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 52 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.0\n",
            "  Downloading torchaudio-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cpu) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cpu) (3.7.4.3)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1+cpu) (7.1.2)\n",
            "Installing collected packages: dataclasses, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0+cpu torchaudio-0.7.0 torchvision-0.8.1+cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UP48BM1rNgc"
      },
      "source": [
        "Next step is to **inport** module. In addition to ***PyTorch*** we will also need ***numpy***.\n",
        "\n",
        "Numpy is essential because it provides:\n",
        "*   **Autograd**: The ability to automatically compute gradients for tensor operations is essential for training deep learning models.\n",
        "*   **GPU support**: While working with massive datasets and large models, PyTorch tensor operations can be performed efficiently using a Graphics Processing Unit (GPU). Computations that might typically take hours can be completed within minutes using GPUs.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jURu3etrTgj"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSrJJBkitnTm"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "*Linear regression* is one of the foundational algorithms in machine learning. In this section we will be creating a model that predicts crop yields for apples and oranges (***target variables***). </br>\n",
        "To achive this we will be computing the average temperature, rainfall, and humidity which are (***input variables or features***). \n",
        "\n",
        "![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n",
        "\n",
        "Linear regression model's target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
        "\n",
        "```\n",
        "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
        "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
        "```\n",
        "\n",
        "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
        "\n",
        "![linear-regression-graph](https://i.imgur.com/4DJ9f8X.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArbmQL2jviX5"
      },
      "source": [
        "Now we need to load data. </br> We have to load verage temperature, rainfall, and humidity (in this order) as ***input variables***. </br> Then we will load crops as ***targets***. </br>\n",
        "Also we are specyfic type of data, in this case is floating point number that is occupying 32 bits in computer memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZ7FnlLv9zV"
      },
      "source": [
        "# Input variables -> in order temperature, rainfall, humidity.\n",
        "inputs = np.array([[73, 67, 43], \n",
        "                   [91, 88, 64], \n",
        "                   [87, 134, 58], \n",
        "                   [102, 43, 37], \n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Targets -> crops in order apples, oranges.\n",
        "targets = np.array([[56, 70], \n",
        "                    [81, 101], \n",
        "                    [119, 133], \n",
        "                    [22, 37], \n",
        "                    [103, 119]], dtype='float32')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDpcHO-PxG-o"
      },
      "source": [
        "Now we are converting arrays to tensors. </br>\n",
        "This is very common aproach since most of data will be in CSV format. </br>\n",
        "It's very easy, we just need to use PyTorch method calles ```torch.from_numpy()``` where we just need to give numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeGr-iRTxYhp"
      },
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFSf6oqtyKkv"
      },
      "source": [
        "At the begginig values of weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices. </br>\n",
        "We will initialized them as random values (and later we will try to find values that are closer to target). </br> The first row of `w` and the first element of `b` are used to predict the first target variable (in this case apples), and similarly, the second for oranges. </br> </br>\n",
        "\n",
        "We will be using `torch.randn()` that creates a tensor with the given shape, with elements picked randomly from a normal distribution with ***mean = 0*** and ***standard deviation 1***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9MxFWoXyoaW"
      },
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-ybGKbzcD0"
      },
      "source": [
        "Our ***model*** is simply a function that performs a matrix multiplication of the `inputs` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n",
        "\n",
        "![matrix-mult](https://i.imgur.com/WGXLFvA.png) </br> </br>\n",
        "`@` represents matrix multiplication in PyTorch, and the `.t` method returns the transpose of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iumPnrw8zhRa"
      },
      "source": [
        "# Model: \n",
        "def model(x):\n",
        "    return x @ w.t() + b"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGKdfpb1zlrn"
      },
      "source": [
        "Now we can predict values using our model and compare them with values from table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4h-S3XxzvsU",
        "outputId": "9f77f517-096b-42a3-b93c-5e17964cfa47"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 89.1041, 241.2144],\n",
            "        [108.4099, 331.3131],\n",
            "        [154.4875, 288.9799],\n",
            "        [ 90.5994, 284.1300],\n",
            "        [ 96.3335, 306.8941]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXyOntIZ0EJ8"
      },
      "source": [
        "Now we will show the actual targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYcQY0qE0JxI",
        "outputId": "63f39eba-b7f9-4cdd-904c-1984bee891af"
      },
      "source": [
        "# actual targets\n",
        "print(targets)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n34SVQP0_FK"
      },
      "source": [
        "There is no similarity and shouldn't be because our model is initialized with random vlaues. </br>\n",
        "To improve it we need to we need a way to *evaluate how well our model is performing*. </br> We can compare the model's predictions with the actual targets using the following method:\n",
        "\n",
        "* Calculate the difference between the two matrices (`preds` and `targets`).\n",
        "* Square all elements of the difference matrix to remove negative values.\n",
        "* Calculate the average of the elements in the resulting matrix.\n",
        "\n",
        "The result is a single number, known as the **mean squared error** (MSE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDTQfjv_1cyT"
      },
      "source": [
        "# MSE loss function\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNHpmCs_1dp6"
      },
      "source": [
        "\n",
        "*   `torch.sum` returns the sum of all the elements in a tensor. \n",
        "*   `.numel` method of a tensor returns the number of elements in a tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TcPQ5w_1zYw",
        "outputId": "88622b54-b8bb-4c76-d286-a7bbc85accd5"
      },
      "source": [
        "# Compute and show loss at this point (model with random values)\n",
        "# Model is better when loss value is as small as possible\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(21092.2539, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm7IGdpf2FXC"
      },
      "source": [
        "With PyTorch, we can automatically compute the gradient (indicating the directions of the fastest increases in the value of a given scalar field) or derivative of the loss to the weights and biases because they have `requires_grad` set to `True`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYTe0qRb276s"
      },
      "source": [
        "The gradients are stored in the .grad property of the respective tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoJpQxQX4xDC"
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdGzu3D43Ft"
      },
      "source": [
        "Step above is very important. We need to ensure to compute gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trIQf0MB2hEG",
        "outputId": "b977a2e1-5853-442e-f851-deab9eb24800"
      },
      "source": [
        "# Gradients for weights compared to weights\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.7201,  0.9764, -0.6742],\n",
            "        [ 1.8216, -0.3591,  3.0881]], requires_grad=True)\n",
            "tensor([[ 2907.0925,  2339.0322,  1461.5016],\n",
            "        [17039.8691, 16260.9307, 10689.0986]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOEF44ag3SL_"
      },
      "source": [
        "## Adjusting weights and biases to reduce the loss\n",
        "\n",
        "The loss is a [quadratic function](https://en.wikipedia.org/wiki/Quadratic_function) of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss with reference to any individual weight or bias element, it will look like the figure shown below.  </br> An important insight from calculus is that the gradient indicates the rate of change of the loss, i.e., the loss function's [slope](https://en.wikipedia.org/wiki/Slope) with reference to the weights and biases.\n",
        "\n",
        "If a gradient element is **positive**:\n",
        "\n",
        "* **increasing** the weight element's value slightly will **increase** the loss\n",
        "* **decreasing** the weight element's value slightly will **decrease** the loss\n",
        "\n",
        "![postive-gradient](https://i.imgur.com/WLzJ4xP.png)\n",
        "\n",
        "If a gradient element is **negative**:\n",
        "\n",
        "* **increasing** the weight element's value slightly will **decrease** the loss\n",
        "* **decreasing** the weight element's value slightly will **increase** the loss\n",
        "\n",
        "![negative=gradient](https://i.imgur.com/dvG2fxU.png)\n",
        "\n",
        "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss with reference to that element. This observation forms the basis of _the gradient descent_ optimization algorithm that we'll use to improve our model (by _descending_ along the _gradient_).\n",
        "\n",
        "# Coding it\n",
        "We can subtract from each weight element a small quantity proportional to the derivative of the loss with reference to that element to reduce the loss slightly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtzMBpAz33aC"
      },
      "source": [
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp6qQC3a34PJ"
      },
      "source": [
        "We multiply the gradients with a very small number (`10^-5` in this case) to ensure that we don't modify the weights by a very large amount. This number is called the ***learning rate*** of the algorithm. \n",
        "\n",
        "We use `torch.no_grad` to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm41kdp4KzP",
        "outputId": "60523032-0c64-43f8-d40a-34df1b67e092"
      },
      "source": [
        "# Applying reducing of loss function\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(21092.2539, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1oJk52e5DA7"
      },
      "source": [
        "Before we proceed, we reset the gradients to zero by invoking the `.zero_()` method. </br> **This is requaired because PyTorch accumulates gradients.** </br>Otherwise, the next time we invoke `.backward` on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmkBB8wN5SiK",
        "outputId": "09a52959-56c9-44e5-874b-29acb9691336"
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G_HGV3s5fKJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz4KqucS5ayt"
      },
      "source": [
        "## Training the model using gradient descent\n",
        "\n",
        "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can _train_ the model using the following steps:\n",
        "\n",
        "1. Generate predictions\n",
        "\n",
        "2. Calculate the loss\n",
        "\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "\n",
        "5. Reset the gradients to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFquaLOi5ad-"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-bib9gB50s2"
      },
      "source": [
        "# Calculate the loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXw0MYA52l2"
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-WuRJ-A54XD"
      },
      "source": [
        "# Now update the weights and biases using the gradients computed above.\n",
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkOSpCQa59bC"
      },
      "source": [
        "# Let's take a look at the new weights and biases\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9jbQNTc6DLA"
      },
      "source": [
        "Now With the new weights and biases, the model should have a lower loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bzs5--_6EVH"
      },
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83hUfAcy6JQF"
      },
      "source": [
        "## Train for multiple epochs\n",
        "\n",
        "To reduce the loss further, we can repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an **_epoch_**.</br> Let's train the model for 100 epochs."
      ]
    }
  ]
}