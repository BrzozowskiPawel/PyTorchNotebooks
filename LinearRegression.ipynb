{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EsHMhSqrnX4"
      },
      "source": [
        "## **Linear Regression**\n",
        "This notebook presents the issue of linear regression along with the explanation of individual sections, enabling an in-depth analysis of this issue. This course is based on aakashns's PyTorch for Deep Learning tutorial [linear-regression](https://jovian.ai/aakashns/02-linear-regression). </br>\n",
        "This notebook will be very similar to one from tutorial. Purpose of it is to help learn linear regression instead of making different notebook. \n",
        "However, some could things will be slightly different.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf4q75nKqwwN"
      },
      "source": [
        "\n",
        "First we need to install the required libraries. The installation of **PyTorch** may differ based on your operating system or hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFj23_C9q1ZZ",
        "outputId": "28623ca1-8842-4176-9b19-34d0b0774148"
      },
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "\n",
        "# Linux / Binder / Colab\n",
        "!pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Windows\n",
        "# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS\n",
        "# !pip install numpy torch torchvision torchaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: torch==1.7.0+cpu in /usr/local/lib/python3.7/dist-packages (1.7.0+cpu)\n",
            "Requirement already satisfied: torchvision==0.8.1+cpu in /usr/local/lib/python3.7/dist-packages (0.8.1+cpu)\n",
            "Requirement already satisfied: torchaudio==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cpu) (0.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cpu) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0+cpu) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1+cpu) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UP48BM1rNgc"
      },
      "source": [
        "Next step is to **inport** module. In addition to ***PyTorch*** we will also need ***numpy***.\n",
        "\n",
        "Numpy is essential because it provides:\n",
        "*   **Autograd**: The ability to automatically compute gradients for tensor operations is essential for training deep learning models.\n",
        "*   **GPU support**: While working with massive datasets and large models, PyTorch tensor operations can be performed efficiently using a Graphics Processing Unit (GPU). Computations that might typically take hours can be completed within minutes using GPUs.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jURu3etrTgj"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSrJJBkitnTm"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "*Linear regression* is one of the foundational algorithms in machine learning. In this section we will be creating a model that predicts crop yields for apples and oranges (***target variables***). </br>\n",
        "To achive this we will be computing the average temperature, rainfall, and humidity which are (***input variables or features***). \n",
        "\n",
        "![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n",
        "\n",
        "Linear regression model's target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a biasÂ :\n",
        "\n",
        "```\n",
        "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
        "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
        "```\n",
        "\n",
        "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
        "\n",
        "![linear-regression-graph](https://i.imgur.com/4DJ9f8X.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArbmQL2jviX5"
      },
      "source": [
        "Now we need to load data. </br> We have to load verage temperature, rainfall, and humidity (in this order) as ***input variables***. </br> Then we will load crops as ***targets***. </br>\n",
        "Also we are specyfic type of data, in this case is floating point number that is occupying 32 bits in computer memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZ7FnlLv9zV"
      },
      "source": [
        "# Input variables -> in order temperature, rainfall, humidity.\n",
        "inputs = np.array([[73, 67, 43], \n",
        "                   [91, 88, 64], \n",
        "                   [87, 134, 58], \n",
        "                   [102, 43, 37], \n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Targets -> crops in order apples, oranges.\n",
        "targets = np.array([[56, 70], \n",
        "                    [81, 101], \n",
        "                    [119, 133], \n",
        "                    [22, 37], \n",
        "                    [103, 119]], dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDpcHO-PxG-o"
      },
      "source": [
        "Now we are converting arrays to tensors. </br>\n",
        "This is very common aproach since most of data will be in CSV format. </br>\n",
        "It's very easy, we just need to use PyTorch method calles ```torch.from_numpy()``` where we just need to give numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeGr-iRTxYhp"
      },
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFSf6oqtyKkv"
      },
      "source": [
        "At the begginig values of weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices. </br>\n",
        "We will initialized them as random values (and later we will try to find values that are closer to target). </br> The first row of `w` and the first element of `b` are used to predict the first target variable (in this case apples), and similarly, the second for oranges. </br> </br>\n",
        "\n",
        "We will be using `torch.randn()` that creates a tensor with the given shape, with elements picked randomly from a normal distribution with ***mean = 0*** and ***standard deviation 1***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9MxFWoXyoaW"
      },
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(2, 3, requires_grad=True)\n",
        "b = torch.randn(2, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-ybGKbzcD0"
      },
      "source": [
        "Our ***model*** is simply a function that performs a matrix multiplication of the `inputs` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n",
        "\n",
        "![matrix-mult](https://i.imgur.com/WGXLFvA.png) </br> </br>\n",
        "`@` represents matrix multiplication in PyTorch, and the `.t` method returns the transpose of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iumPnrw8zhRa"
      },
      "source": [
        "# Model: \n",
        "def model(x):\n",
        "    return x @ w.t() + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGKdfpb1zlrn"
      },
      "source": [
        "Now we can predict values using our model and compare them with values from table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4h-S3XxzvsU",
        "outputId": "7eeea8a4-06ba-4f10-c990-b9507bf7e15d"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[173.1329,  20.7676],\n",
            "        [232.4916,  39.6536],\n",
            "        [259.9453,  30.7304],\n",
            "        [170.9321,   1.7364],\n",
            "        [227.4406,  56.6079]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXyOntIZ0EJ8"
      },
      "source": [
        "Now we will show the actual targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYcQY0qE0JxI",
        "outputId": "c20eb51a-04c3-448e-efb9-09be2387eb2c"
      },
      "source": [
        "# actual targets\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n34SVQP0_FK"
      },
      "source": [
        "There is no similarity and shouldn't be because our model is initialized with random vlaues. </br>\n",
        "To improve it we need to we need a way to *evaluate how well our model is performing*. </br> We can compare the model's predictions with the actual targets using the following method:\n",
        "\n",
        "* Calculate the difference between the two matrices (`preds` and `targets`).\n",
        "* Square all elements of the difference matrix to remove negative values.\n",
        "* Calculate the average of the elements in the resulting matrix.\n",
        "\n",
        "The result is a single number, known as the **mean squared error** (MSE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDTQfjv_1cyT"
      },
      "source": [
        "# MSE loss function\n",
        "def mse(t1, t2):\n",
        "    diff = t1 - t2\n",
        "    return torch.sum(diff * diff) / diff.numel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNHpmCs_1dp6"
      },
      "source": [
        "\n",
        "*   `torch.sum` returns the sum of all the elements in a tensor. \n",
        "*   `.numel` method of a tensor returns the number of elements in a tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TcPQ5w_1zYw",
        "outputId": "664e93d4-a425-4ce4-9a53-b47ab8afe6e9"
      },
      "source": [
        "# Compute and show loss at this point (model with random values)\n",
        "# Model is better when loss value is as small as possible\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(11598.4160, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm7IGdpf2FXC"
      },
      "source": [
        "With PyTorch, we can automatically compute the gradient (indicating the directions of the fastest increases in the value of a given scalar field) or derivative of the loss to the weights and biases because they have `requires_grad` set to `True`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYTe0qRb276s"
      },
      "source": [
        "The gradients are stored in the .grad property of the respective tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoJpQxQX4xDC"
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdGzu3D43Ft"
      },
      "source": [
        "Step above is very important. We need to ensure to compute gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trIQf0MB2hEG",
        "outputId": "250bda9c-d4be-436b-f587-b59073b0f1ef"
      },
      "source": [
        "# Gradients for weights compared to weights\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.8724,  0.8349,  1.2439],\n",
            "        [-0.4301, -0.0584,  1.3264]], requires_grad=True)\n",
            "tensor([[11675.2285, 11683.2393,  7425.6650],\n",
            "        [-5195.1753, -5981.4297, -3529.3989]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOEF44ag3SL_"
      },
      "source": [
        "## Adjusting weights and biases to reduce the loss\n",
        "\n",
        "The loss is a [quadratic function](https://en.wikipedia.org/wiki/Quadratic_function) of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss with reference to any individual weight or bias element, it will look like the figure shown below.  </br> An important insight from calculus is that the gradient indicates the rate of change of the loss, i.e., the loss function's [slope](https://en.wikipedia.org/wiki/Slope) with reference to the weights and biases.\n",
        "\n",
        "If a gradient element is **positive**:\n",
        "\n",
        "* **increasing** the weight element's value slightly will **increase** the loss\n",
        "* **decreasing** the weight element's value slightly will **decrease** the loss\n",
        "\n",
        "![postive-gradient](https://i.imgur.com/WLzJ4xP.png)\n",
        "\n",
        "If a gradient element is **negative**:\n",
        "\n",
        "* **increasing** the weight element's value slightly will **decrease** the loss\n",
        "* **decreasing** the weight element's value slightly will **increase** the loss\n",
        "\n",
        "![negative=gradient](https://i.imgur.com/dvG2fxU.png)\n",
        "\n",
        "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss with reference to that element. This observation forms the basis of _the gradient descent_ optimization algorithm that we'll use to improve our model (by _descending_ along the _gradient_).\n",
        "\n",
        "# Coding it\n",
        "We can subtract from each weight element a small quantity proportional to the derivative of the loss with reference to that element to reduce the loss slightly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtzMBpAz33aC"
      },
      "source": [
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp6qQC3a34PJ"
      },
      "source": [
        "We multiply the gradients with a very small number (`10^-5` in this case) to ensure that we don't modify the weights by a very large amount. This number is called the ***learning rate*** of the algorithm.Â \n",
        "\n",
        "We use `torch.no_grad` to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm41kdp4KzP",
        "outputId": "3b059e86-0666-44f4-9ef6-8bde1d76bf56"
      },
      "source": [
        "# Applying reducing of loss function\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(11598.4160, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1oJk52e5DA7"
      },
      "source": [
        "Before we proceed, we reset the gradients to zero by invoking theÂ `.zero_()` method. </br> **This is requaired because PyTorch accumulates gradients.** </br>Otherwise, the next time we invoke `.backward` on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmkBB8wN5SiK",
        "outputId": "baf7b931-56b9-41f0-c94e-fb6de24d81c0"
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G_HGV3s5fKJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz4KqucS5ayt"
      },
      "source": [
        "## Training the model using gradient descent\n",
        "\n",
        "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can _train_ the model using the following steps:\n",
        "\n",
        "1. Generate predictions\n",
        "\n",
        "2. Calculate the loss\n",
        "\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "\n",
        "5. Reset the gradients to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFquaLOi5ad-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edae6be-8d26-4fe7-a34a-245a359619c3"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[153.5878,  30.0859],\n",
            "        [206.8321,  51.9043],\n",
            "        [229.8240,  45.3130],\n",
            "        [151.2507,  10.9140],\n",
            "        [202.9694,  68.4059]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-bib9gB50s2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e175c486-046a-48b6-ac5b-858850f10028"
      },
      "source": [
        "# Calculate the loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7927.1431, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXw0MYA52l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1881ec-e1cb-416c-d6b8-a8525baa1096"
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 9659.5547,  9523.3721,  6091.4902],\n",
            "        [-4232.3926, -4944.6895, -2890.2083]])\n",
            "tensor([112.6928, -50.6754])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-WuRJ-A54XD"
      },
      "source": [
        "# Now update the weights and biases using the gradients computed above.\n",
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkOSpCQa59bC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c3c25f-db75-4f83-c0b2-4872a728e590"
      },
      "source": [
        "# Let's take a look at the new weights and biases\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6591,  0.6229,  1.1087],\n",
            "        [-0.3359,  0.0509,  1.3906]], requires_grad=True)\n",
            "tensor([ 0.0157, -0.9552], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9jbQNTc6DLA"
      },
      "source": [
        "Now With the new weights and biases, the model should have a lower loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bzs5--_6EVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d5db84-de32-4f57-9d3c-280a28d5d7ae"
      },
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5451.8154, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83hUfAcy6JQF"
      },
      "source": [
        "## Train for multipleÂ epochs\n",
        "\n",
        "To reduce the loss further, we can repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an **_epoch_**.</br> Let's train the model for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvxUHAM_tF97"
      },
      "source": [
        "# Train for 100 epochs\n",
        "for i in range(100):\n",
        "    preds = model(inputs)\n",
        "    loss = mse(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETo4e1j8tJ0g",
        "outputId": "f07c4abb-1fca-491a-8595-2e5407795ebc"
      },
      "source": [
        "# Calculate loss after 100 epochs\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(111.3868, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQs13lTYtGte"
      },
      "source": [
        "As we can see loss in now much lower then at the beggining. </br>\n",
        "Lets compare predictions and targets:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKls1nxatVJd",
        "outputId": "495951da-d8f9-45d4-c93e-75cbcc39e58e"
      },
      "source": [
        "# Predictions\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 60.8015,  71.3727],\n",
              "        [ 86.5965, 106.5045],\n",
              "        [102.8638, 117.9762],\n",
              "        [ 42.4033,  44.6409],\n",
              "        [ 97.1159, 124.6495]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy11BwEitjuy",
        "outputId": "1d8c398d-1b9a-4103-fcb8-b035296f14a1"
      },
      "source": [
        "# Targets\n",
        "targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laGE-50AuxoB"
      },
      "source": [
        "## Linear Regression with build in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c21DUadxvNc5"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alR20Z8VvOUD"
      },
      "source": [
        "As before, we are loading all nessesery data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy47qMXDwAYR"
      },
      "source": [
        "# Input (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43], \n",
        "                   [91, 88, 64], \n",
        "                   [87, 134, 58], \n",
        "                   [102, 43, 37], \n",
        "                   [69, 96, 70], \n",
        "                   [74, 66, 43], \n",
        "                   [91, 87, 65], \n",
        "                   [88, 134, 59], \n",
        "                   [101, 44, 37], \n",
        "                   [68, 96, 71], \n",
        "                   [73, 66, 44], \n",
        "                   [92, 87, 64], \n",
        "                   [87, 135, 57], \n",
        "                   [103, 43, 36], \n",
        "                   [68, 97, 70]], \n",
        "                  dtype='float32')\n",
        "\n",
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70], \n",
        "                    [81, 101], \n",
        "                    [119, 133], \n",
        "                    [22, 37], \n",
        "                    [103, 119],\n",
        "                    [57, 69], \n",
        "                    [80, 102], \n",
        "                    [118, 132], \n",
        "                    [21, 38], \n",
        "                    [104, 118], \n",
        "                    [57, 69], \n",
        "                    [82, 100], \n",
        "                    [118, 134], \n",
        "                    [20, 38], \n",
        "                    [102, 120]], \n",
        "                   dtype='float32')\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRghs20cwImJ"
      },
      "source": [
        "## Dataset and DataLoader\n",
        "\n",
        "We'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEp_pMzV0OiS"
      },
      "source": [
        "# import functionality\n",
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFb2a0GFwPuj",
        "outputId": "66226516-4b9f-4cd2-f476-fc21cd7648b9"
      },
      "source": [
        "# Define dataset\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]), tensor([[ 56.,  70.],\n",
              "         [ 81., 101.],\n",
              "         [119., 133.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WilakVH4xHuY"
      },
      "source": [
        "The `TensorDataset` allows us to access a small section of the training data using the array indexing notation (`[0:3]` in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
        "</br></br>\n",
        "We'll also create a `DataLoader`, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvIJM79gxTZv"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Define data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaLQmK1nxagC"
      },
      "source": [
        "We can use the data loader in a for loop. Let's look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjPF3FC4xar6",
        "outputId": "d325a363-2aca-422d-e278-fc6c84f29522"
      },
      "source": [
        "for xb, yb in train_dl:\n",
        "    print(xb)\n",
        "    print(yb)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 88., 134.,  59.],\n",
            "        [ 74.,  66.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 135.,  57.],\n",
            "        [ 87., 134.,  58.]])\n",
            "tensor([[118., 132.],\n",
            "        [ 57.,  69.],\n",
            "        [ 81., 101.],\n",
            "        [118., 134.],\n",
            "        [119., 133.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbVP1GsZxgPZ"
      },
      "source": [
        "In each iteration, the data loader returns one batch of data with the given batch size. </br>If `shuffle` is set to `True`, it shuffles the training data before creating batches. </br>*Shuffling helps randomize the input to the optimization algorithm, leading to a* ***faster reduction in the loss***.\n",
        "</br> </br>\n",
        "## nn.Linear\n",
        "\n",
        "Instead of initializing the weights & biases manually, we can define the model using the `nn.Linear` class from PyTorch, which does it automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq_XYviFyF7H",
        "outputId": "4098c7bf-2b77-4c89-b0cf-c80c602485fc"
      },
      "source": [
        "# Define model\n",
        "model = nn.Linear(3, 2)\n",
        "print(model.weight)\n",
        "print(model.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.5124,  0.1387,  0.1039],\n",
            "        [-0.2434, -0.1633,  0.1130]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.2457, -0.5621], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-0bg31yGy4"
      },
      "source": [
        "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3ipCf-aysyL",
        "outputId": "83fb0b32-bef7-45cc-c830-53a03ff463a7"
      },
      "source": [
        "# Parameters\n",
        "list(model.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.5124,  0.1387,  0.1039],\n",
              "         [-0.2434, -0.1633,  0.1130]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.2457, -0.5621], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFXWlEgAyvhX",
        "outputId": "9b8ea577-a6b5-4cc9-c348-1115ceae37b7"
      },
      "source": [
        "# Generate predictions\n",
        "# We can use model just as previously\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-23.3995, -24.4121],\n",
              "        [-27.5283, -29.8494],\n",
              "        [-19.7238, -37.0634],\n",
              "        [-42.2098, -28.2316],\n",
              "        [-14.5232, -25.1219],\n",
              "        [-24.0505, -24.4923],\n",
              "        [-27.5630, -29.5731],\n",
              "        [-20.1323, -37.1938],\n",
              "        [-41.5588, -28.1514],\n",
              "        [-13.9069, -24.7654],\n",
              "        [-23.4342, -24.1359],\n",
              "        [-28.1793, -29.9296],\n",
              "        [-19.6890, -37.3396],\n",
              "        [-42.8261, -28.5880],\n",
              "        [-13.8721, -25.0417]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqLsAvnKy37m"
      },
      "source": [
        "## Loss Function\n",
        "\n",
        "Instead of defining a loss function manually, we can use the built-in loss function `mse_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjF2I9Oiy2tN",
        "outputId": "611037ce-dd87-442c-d2b4-8ddab1132712"
      },
      "source": [
        "# Import nn.functional\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "# Compute the loss for the current predictions of our model.\n",
        "loss = loss_fn(model(inputs), targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(13501.3926, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyeDzFnmzFjb"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer `optim.SGD`. </br> SGD is short for \"stochastic gradient descent\". The term **_stochastic_ indicates** that samples are selected in random batches instead of as a single group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kKOLC6EzPrt"
      },
      "source": [
        "# Define optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1_Ajdl7zScK"
      },
      "source": [
        "Note that model.parameters() is passed as an argument to optim.SGD so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate that controls the amount by which the parameters are modified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9uOOQbAzcD-"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We are now ready to train the model. We'll follow the same process to implement gradient descent:\n",
        "\n",
        "1. Generate predictions\n",
        "\n",
        "2. Calculate the loss\n",
        "\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "\n",
        "5. Reset the gradients to zero\n",
        "\n",
        "The only change is that we'll work batches of data instead of processing the entire training data in every iteration. Let's define a utility function `fit` that trains the model for a given number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li08c7ObzhKN"
      },
      "source": [
        "# Utility function to train the model\n",
        "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    \n",
        "    # Repeat for given number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        # Train with batches of data\n",
        "        for xb,yb in train_dl:\n",
        "            \n",
        "            # 1. Generate predictions\n",
        "            pred = model(xb)\n",
        "            \n",
        "            # 2. Calculate loss\n",
        "            loss = loss_fn(pred, yb)\n",
        "            \n",
        "            # 3. Compute gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # 4. Update parameters using gradients\n",
        "            opt.step()\n",
        "            \n",
        "            # 5. Reset the gradients to zero\n",
        "            opt.zero_grad()\n",
        "        \n",
        "        # Print the progress\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSjVZ0a1zm8I"
      },
      "source": [
        "Some things to note above:\n",
        "\n",
        "* We use the data loader defined earlier to get batches of data for every iteration.\n",
        "\n",
        "* Instead of updating parameters (weights and biases) manually, we use `opt.step` to perform the update and `opt.zero_grad` to reset the gradients to zero.\n",
        "\n",
        "* We've also added a log statement that prints the loss from the last batch of data for every 10th epoch to track training progress. `loss.item` returns the actual value stored in the loss tensor.\n",
        "\n",
        "Let's train the model for 100 epochs and compare resoults."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA8uxAGQzvKs",
        "outputId": "10e4b937-c3b5-401c-abb9-cdef824af48e"
      },
      "source": [
        "fit(100, model, loss_fn, opt, train_dl)\n",
        "\n",
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/100], Loss: 143.0284\n",
            "Epoch [20/100], Loss: 172.9396\n",
            "Epoch [30/100], Loss: 84.4990\n",
            "Epoch [40/100], Loss: 73.8386\n",
            "Epoch [50/100], Loss: 31.1499\n",
            "Epoch [60/100], Loss: 53.3417\n",
            "Epoch [70/100], Loss: 40.2082\n",
            "Epoch [80/100], Loss: 21.7125\n",
            "Epoch [90/100], Loss: 12.9748\n",
            "Epoch [100/100], Loss: 7.0584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 57.5894,  71.2395],\n",
              "        [ 81.0094,  99.2403],\n",
              "        [119.2657, 133.8689],\n",
              "        [ 24.3217,  43.4286],\n",
              "        [ 98.1019, 112.8588],\n",
              "        [ 56.3990,  70.2701],\n",
              "        [ 80.6553,  99.0531],\n",
              "        [119.4640, 134.3583],\n",
              "        [ 25.5121,  44.3979],\n",
              "        [ 98.9382, 113.6409],\n",
              "        [ 57.2352,  71.0523],\n",
              "        [ 79.8190,  98.2710],\n",
              "        [119.6198, 134.0560],\n",
              "        [ 23.4854,  42.6464],\n",
              "        [ 99.2923, 113.8281]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bx_wGQ8z4dS",
        "outputId": "4dea13ba-5973-4c04-9576-959257428f6a"
      },
      "source": [
        "# Compare with targets\n",
        "targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.],\n",
              "        [ 57.,  69.],\n",
              "        [ 80., 102.],\n",
              "        [118., 132.],\n",
              "        [ 21.,  38.],\n",
              "        [104., 118.],\n",
              "        [ 57.,  69.],\n",
              "        [ 82., 100.],\n",
              "        [118., 134.],\n",
              "        [ 20.,  38.],\n",
              "        [102., 120.]])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu0TmMVc_eId"
      },
      "source": [
        "The final result is very good. </br> The loss value is just 7. Values of predictions are similar to the targets."
      ]
    }
  ]
}